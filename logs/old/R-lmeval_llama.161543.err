/var/spool/slurm/d/job161543/slurm_script: line 16: activate: No such file or directory
2023-12-13:14:38:31,062 INFO     [utils.py:148] Note: NumExpr detected 32 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2023-12-13:14:38:31,062 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
2023-12-13:14:38:31,791 INFO     [instantiator.py:21] Created a temporary directory at /tmp/tmpe8iwgbxw
2023-12-13:14:38:31,791 INFO     [instantiator.py:76] Writing /tmp/tmpe8iwgbxw/_remote_module_non_scriptable.py
2023-12-13:14:38:34,565 WARNING  [logging.py:60] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2023-12-13:14:38:34,565 INFO     [huggingface.py:120] Using device 'cuda'
2023-12-13:14:38:55,871 INFO     [__init__.py:194] Available tasks:
2023-12-13:14:38:55,871 INFO     [__init__.py:195] ['squadv2', 'scrolls_qasper', 'scrolls_quality', 'scrolls_narrativeqa', 'scrolls_contractnli', 'scrolls_govreport', 'scrolls_summscreenfd', 'scrolls_qmsum']
Traceback (most recent call last):
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 192, in get_task
    return TASK_REGISTRY[task_name](config=config)
           ~~~~~~~~~~~~~^^^^^^^^^^^
KeyError: 'headqa_en'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/home/jocl/lmeval/eval_single.py", line 44, in <module>
    eval_model(model_name, model_path)
  File "/home/jocl/lmeval/eval_single.py", line 22, in eval_model
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/utils.py", line 404, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/evaluator.py", line 124, in simple_evaluate
    task_dict = lm_eval.tasks.get_task_dict(tasks)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 250, in get_task_dict
    task_name: get_task(task_name=task_element, config=config),
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/tasks/__init__.py", line 196, in get_task
    raise KeyError(f"Missing task {task_name}")
KeyError: 'Missing task headqa_en'

real	0m28.206s
user	0m6.822s
sys	0m16.114s
