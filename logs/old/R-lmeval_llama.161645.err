/var/spool/slurm/d/job161645/slurm_script: line 18: activate: No such file or directory
2023-12-14:12:09:25,562 INFO     [utils.py:148] Note: NumExpr detected 48 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
2023-12-14:12:09:25,562 INFO     [utils.py:160] NumExpr defaulting to 8 threads.
2023-12-14:12:09:27,532 INFO     [instantiator.py:21] Created a temporary directory at /tmp/tmpzkrwuj7t
2023-12-14:12:09:27,532 INFO     [instantiator.py:76] Writing /tmp/tmpzkrwuj7t/_remote_module_non_scriptable.py
2023-12-14:12:09:29,876 INFO     [__main__.py:132] Verbosity set to INFO
2023-12-14:12:09:35,834 INFO     [__main__.py:205] Selected Tasks: ['know_dist']
2023-12-14:12:09:35,837 WARNING  [evaluator.py:93] generation_kwargs specified through cli, these settings will be used over set parameters in yaml tasks.
2023-12-14:12:09:35,900 WARNING  [logging.py:60] Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
2023-12-14:12:09:35,901 INFO     [huggingface.py:120] Using device 'cuda:0'
Traceback (most recent call last):
  File "/home/jocl/LMOps/minillm/transformers/src/transformers/utils/hub.py", line 431, in cached_file
    resolved_file = hf_hub_download(
                    ^^^^^^^^^^^^^^^^
  File "/home/jocl/.conda/envs/lmeval/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 110, in _inner_fn
    validate_repo_id(arg_value)
  File "/home/jocl/.conda/envs/lmeval/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 158, in validate_repo_id
    raise HFValidationError(
huggingface_hub.utils._validators.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '{/home/data_shares/mapillary/lmeval_models/sft/llama-13B}'. Use `repo_type` argument if needed.

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/jocl/.conda/envs/lmeval/bin/lm_eval", line 8, in <module>
    sys.exit(cli_evaluate())
             ^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/__main__.py", line 207, in cli_evaluate
    results = evaluator.simple_evaluate(
              ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/utils.py", line 404, in _wrapper
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/evaluator.py", line 102, in simple_evaluate
    lm = lm_eval.api.registry.get_model(model).create_from_arg_string(
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/api/model.py", line 136, in create_from_arg_string
    return cls(**args, **args2)
           ^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/lmeval/lm-evaluation-harness/lm_eval/models/huggingface.py", line 155, in __init__
    self._config = transformers.AutoConfig.from_pretrained(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/LMOps/minillm/transformers/src/transformers/models/auto/configuration_auto.py", line 1054, in from_pretrained
    config_dict, unused_kwargs = PretrainedConfig.get_config_dict(pretrained_model_name_or_path, **kwargs)
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/LMOps/minillm/transformers/src/transformers/configuration_utils.py", line 622, in get_config_dict
    config_dict, kwargs = cls._get_config_dict(pretrained_model_name_or_path, **kwargs)
                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/jocl/LMOps/minillm/transformers/src/transformers/configuration_utils.py", line 677, in _get_config_dict
    resolved_config_file = cached_file(
                           ^^^^^^^^^^^^
  File "/home/jocl/LMOps/minillm/transformers/src/transformers/utils/hub.py", line 496, in cached_file
    raise EnvironmentError(
OSError: Incorrect path_or_model_id: '{/home/data_shares/mapillary/lmeval_models/sft/llama-13B}'. Please provide either the path to a local folder or the repo_id of a model on the Hub.
