#!/bin/bash

#SBATCH --account=researchers
#SBATCH --job-name=lmeval    # Job name
#SBATCH --output=logs/R-%x.%j.out      # Name of output file (%j expands to jobId)
#SBATCH --error=logs/R-%x.%j.err     # Error handling
#SBATCH --nodes=1                # Total number of nodes requested
#SBATCH --cpus-per-task=8        # Schedule 8 cores (includes hyperthreading)
#SBATCH --mem=48G
#SBATCH --constraint="gpu_rtx8000|gpu_a100_40gb|gpu_v100" # Use either a v100 or a100
#SBATCH --gres=gpu:1      #v100:1 or a100_40gb:1 on brown
#SBATCH --time=1-00:00:00          # Run time (hh:mm:ss) 
#SBATCH --partition=red,brown    # Run on either the red or brown queue

#srun hostname

# module load Anaconda3
source activate lmeval # Not working???

nvidia-smi

# model_name="llama_train_sft_13b"
# model_path="/home/data_shares/mapillary/lmeval_models/sft/llama-13B"

# model_name="llamaSpanish256"
# model_path="/home/data_shares/mapillary/lmeval_models/llamaSpanish256"

# model_name="llama_train_sft_7b"
# model_path="/home/data_shares/mapillary/lmeval_models/sft/llama-7B"




# model_path="/home/data_shares/mapillary/bloom_3b_7b_both_500it"
# model_name="bloom_3b_7b_both_500it" # DONE
# model_path="/home/data_shares/mapillary/bloom_3b_7b_both_1000it"
# model_name="bloom_3b_7b_both_1000it" # DONE
# model_path="/home/data_shares/mapillary/bloom_3b_7b_english_500it"
# model_name="bloom_3b_7b_english_500it" # DONE
# model_path="/home/data_shares/mapillary/bloom_3b_7b_english_1000it"
# model_name="bloom_3b_7b_english_1000it" # DONE
# model_path="/home/data_shares/mapillary/bloom_3b_7b_spanish_500it"
# model_name="bloom_3b_7b_spanish_500it" # DONE
# model_path="/home/data_shares/mapillary/bloom_3b_7b_spanish_1000it"
# model_name="bloom_3b_7b_spanish_1000it"
# model_path="/home/data_shares/mapillary/llama_7b_13b_both_500it"
# model_name="llama_7b_13b_both_500it"
# model_path="/home/data_shares/mapillary/llama_7b_13b_both_1000it"
# model_name="llama_7b_13b_both_1000it"
# model_path="/home/data_shares/mapillary/llama_7b_13b_english_500it"
# model_name="llama_7b_13b_english_500it" # running
# model_path="/home/data_shares/mapillary/llama_7b_13b_english_1000it"
# model_name="llama_7b_13b_english_1000it" # Malformed ???
# model_path="/home/data_shares/mapillary/llama_7b_13b_spanish_500it"
# model_name="llama_7b_13b_spanish_500it" # Malformed ???
# model_path="/home/data_shares/mapillary/llama_7b_13b_spanish_1000it"
# model_name="llama_7b_13b_spanish_1000it" # running

echo $model_name

lm_eval --model hf --model_args "pretrained=$model_path" --tasks "know_dist" --batch_size auto --max_batch_size 64 --use_cache="lm_cache/$model_name" --device cuda:0 --output_path "results/$model_name" --num_fewshot 0


# bloom-1b1 bloom-560m bloom-7b1 bloom-3B
# time python3 eval_single.py "bigscience/bloom-3B"